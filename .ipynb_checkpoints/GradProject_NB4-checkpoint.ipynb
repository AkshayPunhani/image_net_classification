{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> DS200A Computer Vision Assignment</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>  Part Four: Extension Activities</h2>\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow (Optional)- Now, try using TensorFlow to categorize your images. The accuracy should be significantly higher due to the usage of nueral nets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 960 samples, validate on 240 samples\n",
      "Epoch 1/50\n",
      "960/960 [==============================] - 13s 13ms/step - loss: 0.6028 - acc: 0.6771 - val_loss: 0.4095 - val_acc: 0.9435\n",
      "Epoch 2/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.4633 - acc: 0.7965 - val_loss: 0.3425 - val_acc: 0.9173\n",
      "Epoch 3/50\n",
      "960/960 [==============================] - 11s 12ms/step - loss: 0.4293 - acc: 0.8227 - val_loss: 0.3270 - val_acc: 0.9150\n",
      "Epoch 4/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.4040 - acc: 0.8395 - val_loss: 0.3032 - val_acc: 0.9500\n",
      "Epoch 5/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.3785 - acc: 0.8526 - val_loss: 0.2682 - val_acc: 0.9500\n",
      "Epoch 6/50\n",
      "960/960 [==============================] - 11s 11ms/step - loss: 0.3601 - acc: 0.8652 - val_loss: 0.2518 - val_acc: 0.9500\n",
      "Epoch 7/50\n",
      "960/960 [==============================] - 11s 12ms/step - loss: 0.3427 - acc: 0.8751 - val_loss: 0.2288 - val_acc: 0.9500\n",
      "Epoch 8/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.3453 - acc: 0.8812 - val_loss: 0.2218 - val_acc: 0.9500\n",
      "Epoch 9/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.3132 - acc: 0.8936 - val_loss: 0.2161 - val_acc: 0.9500\n",
      "Epoch 10/50\n",
      "960/960 [==============================] - 11s 11ms/step - loss: 0.3080 - acc: 0.9018 - val_loss: 0.2150 - val_acc: 0.9500\n",
      "Epoch 11/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.2966 - acc: 0.9049 - val_loss: 0.2194 - val_acc: 0.9500\n",
      "Epoch 12/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.2925 - acc: 0.9114 - val_loss: 0.2307 - val_acc: 0.9504\n",
      "Epoch 13/50\n",
      "960/960 [==============================] - 12s 12ms/step - loss: 0.2776 - acc: 0.9153 - val_loss: 0.2076 - val_acc: 0.9502\n",
      "Epoch 14/50\n",
      "960/960 [==============================] - 11s 11ms/step - loss: 0.2648 - acc: 0.9197 - val_loss: 0.2000 - val_acc: 0.9508\n",
      "Epoch 15/50\n",
      "960/960 [==============================] - 11s 11ms/step - loss: 0.2615 - acc: 0.9195 - val_loss: 0.2040 - val_acc: 0.9504\n",
      "Epoch 16/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.2572 - acc: 0.9261 - val_loss: 0.2002 - val_acc: 0.9500\n",
      "Epoch 17/50\n",
      "960/960 [==============================] - 11s 11ms/step - loss: 0.2633 - acc: 0.9297 - val_loss: 0.1973 - val_acc: 0.9504\n",
      "Epoch 18/50\n",
      "960/960 [==============================] - 11s 12ms/step - loss: 0.2373 - acc: 0.9332 - val_loss: 0.1935 - val_acc: 0.9508\n",
      "Epoch 19/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.2304 - acc: 0.9357 - val_loss: 0.1954 - val_acc: 0.9508\n",
      "Epoch 20/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.2336 - acc: 0.9332 - val_loss: 0.1989 - val_acc: 0.9515\n",
      "Epoch 21/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.2327 - acc: 0.9355 - val_loss: 0.1853 - val_acc: 0.9510\n",
      "Epoch 22/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.2179 - acc: 0.9387 - val_loss: 0.1872 - val_acc: 0.9506\n",
      "Epoch 23/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.2316 - acc: 0.9380 - val_loss: 0.1832 - val_acc: 0.9513\n",
      "Epoch 24/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.2070 - acc: 0.9421 - val_loss: 0.1785 - val_acc: 0.9504\n",
      "Epoch 25/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.2125 - acc: 0.9420 - val_loss: 0.1865 - val_acc: 0.9519\n",
      "Epoch 26/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.2135 - acc: 0.9429 - val_loss: 0.1791 - val_acc: 0.9519\n",
      "Epoch 27/50\n",
      "960/960 [==============================] - 11s 11ms/step - loss: 0.1996 - acc: 0.9447 - val_loss: 0.1810 - val_acc: 0.9521\n",
      "Epoch 28/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.2043 - acc: 0.9450 - val_loss: 0.1731 - val_acc: 0.9521\n",
      "Epoch 29/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.1941 - acc: 0.9438 - val_loss: 0.1779 - val_acc: 0.9529\n",
      "Epoch 30/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.1941 - acc: 0.9470 - val_loss: 0.1830 - val_acc: 0.9519\n",
      "Epoch 31/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.1903 - acc: 0.9451 - val_loss: 0.1714 - val_acc: 0.9535\n",
      "Epoch 32/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.1816 - acc: 0.9486 - val_loss: 0.1708 - val_acc: 0.9535\n",
      "Epoch 33/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.1795 - acc: 0.9489 - val_loss: 0.1748 - val_acc: 0.9540\n",
      "Epoch 34/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.1820 - acc: 0.9492 - val_loss: 0.1729 - val_acc: 0.9546\n",
      "Epoch 35/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.1783 - acc: 0.9489 - val_loss: 0.1713 - val_acc: 0.9542\n",
      "Epoch 36/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.1744 - acc: 0.9491 - val_loss: 0.1730 - val_acc: 0.9554\n",
      "Epoch 37/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.1739 - acc: 0.9499 - val_loss: 0.1718 - val_acc: 0.9552\n",
      "Epoch 38/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.1677 - acc: 0.9522 - val_loss: 0.1677 - val_acc: 0.9548\n",
      "Epoch 39/50\n",
      "960/960 [==============================] - 11s 11ms/step - loss: 0.1726 - acc: 0.9504 - val_loss: 0.1706 - val_acc: 0.9550\n",
      "Epoch 40/50\n",
      "960/960 [==============================] - 11s 11ms/step - loss: 0.1696 - acc: 0.9506 - val_loss: 0.1718 - val_acc: 0.9554\n",
      "Epoch 41/50\n",
      "960/960 [==============================] - 11s 11ms/step - loss: 0.1639 - acc: 0.9534 - val_loss: 0.1674 - val_acc: 0.9556\n",
      "Epoch 42/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.1557 - acc: 0.9514 - val_loss: 0.1706 - val_acc: 0.9556\n",
      "Epoch 43/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.1571 - acc: 0.9532 - val_loss: 0.1690 - val_acc: 0.9558\n",
      "Epoch 44/50\n",
      "960/960 [==============================] - 11s 11ms/step - loss: 0.1570 - acc: 0.9521 - val_loss: 0.1655 - val_acc: 0.9550\n",
      "Epoch 45/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.1532 - acc: 0.9536 - val_loss: 0.1688 - val_acc: 0.9560\n",
      "Epoch 46/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.1583 - acc: 0.9541 - val_loss: 0.1667 - val_acc: 0.9563\n",
      "Epoch 47/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.1498 - acc: 0.9549 - val_loss: 0.1688 - val_acc: 0.9554\n",
      "Epoch 48/50\n",
      "960/960 [==============================] - 11s 12ms/step - loss: 0.1417 - acc: 0.9542 - val_loss: 0.1789 - val_acc: 0.9554\n",
      "Epoch 49/50\n",
      "960/960 [==============================] - 10s 11ms/step - loss: 0.1389 - acc: 0.9543 - val_loss: 0.2240 - val_acc: 0.9519\n",
      "Epoch 50/50\n",
      "960/960 [==============================] - 11s 11ms/step - loss: 0.1534 - acc: 0.9544 - val_loss: 0.2146 - val_acc: 0.9533\n",
      "301/301 [==============================] - 1s 4ms/step\n",
      "Accuracy on test data:  0.9524916323316454\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from skimage.color import rgb2gray\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def TF(X_train,y_train):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), input_shape=(105, 105, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "    model.fit(X_train,y_train,batch_size=250,epochs=50,verbose=1,validation_split=0.2)\n",
    "    return model\n",
    "\n",
    "#Read data from file\n",
    "data_file = Path(\"data/NB_1\", \"cleaned_data.hdf\")\n",
    "data_from_nb1 = pd.read_hdf(data_file, \"starting_data\")\n",
    "\n",
    "#Tranform into grayscale(as not all images are coloured) and convert it into same size\n",
    "nn_input = data_from_nb1.apply(lambda x: resize(rgb2gray(x['pictures']), (105, 105)), axis=1)\n",
    "X = np.array([list(i.reshape(105,105,1)) for i in nn_input])\n",
    "y = pd.get_dummies(data_from_nb1['encoding'])\n",
    "\n",
    "#Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.20, random_state =42)\n",
    "\n",
    "#Fit the model\n",
    "model_nn = TF(X_train,y_train)\n",
    "\n",
    "scores = model_nn.evaluate(X_test, y_test)\n",
    "print('Accuracy on test data: ', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: FutureWarning: 'argmax' is deprecated, use 'idxmax' instead. The behavior of 'argmax'\n",
      "will be corrected to return the positional maximum in the future.\n",
      "Use 'series.values.argmax' to get the position of the maximum now.\n",
      "  return getattr(obj, method)(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved output to:  validation_output_neural_network\n"
     ]
    }
   ],
   "source": [
    "from skimage import io\n",
    "import glob\n",
    "\n",
    "#Read Validation Input\n",
    "validation_path = '20_Validation/'\n",
    "file_list= os.listdir(validation_path)\n",
    "image_list = [io.imread(validation_path + animal) for animal in file_list if os.path.isfile(validation_path + animal)]\n",
    "validation_df = pd.DataFrame(image_list, columns=['pictures'])\n",
    "\n",
    "#Tranform input\n",
    "val_input = validation_df.apply(lambda x: resize(rgb2gray(x['pictures']), (105, 105)), axis=1)\n",
    "X_val = np.array([list(i.reshape(105,105,1)) for i in val_input])\n",
    "\n",
    "# Predict using model\n",
    "res = model_nn.predict(X_val)\n",
    "\n",
    "#Outputting the to a csv\n",
    "output_file_name = 'validation_output_neural_network'\n",
    "pd.DataFrame(res).apply(lambda x: np.argmax(x), axis=1).to_csv(output_file_name)\n",
    "print('Saved output to: ', output_file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
